{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z71ZL2DPE_oS"
      },
      "source": [
        "# Language Modeling\n",
        "\n",
        "Изучим мощь рекуррентных сетей на примере решения задачи LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co-TRbmQFuHm"
      },
      "source": [
        "# Language Modeling\n",
        "\n",
        "Давайте введём понятия, которые нам сегодня пригодятся"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDceUK8mqHG0"
      },
      "source": [
        "Сами по себе языковые модели предназначены для того, чтобы как-то оценивать вероятность некоторой языковой конструкции. Например, мы можем свести к вероятностной модели вероятность последовательности токенов следующим образом:\n",
        "\n",
        "$$p(x) = \\prod_{i=1}^{N}p(x_i|x_1, ..., x_{i-1}) = p(x_1) * p(x_2 | x_1) * p(x_3 | x1, x2) * ...$$\n",
        "$$\\log p(x) = \\sum_{i=1}^{N}\\log p(x_i|x_1, ..., x_{i-1})$$\n",
        "\n",
        "Если мы обучим нашу модель предсказывать эти вероятности, то сможем не только предсказывать вероятность последовательности, но и получать вероятности следующих токенов. Последнее - это не что иное, как генерация текста."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eK1sdmfGz7v"
      },
      "source": [
        "# Average ArXiv Enjoyer\n",
        "\n",
        "На одной из прошлых пар мы научились правильно читать статьи, а теперь давайте научимся их писать"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq2cwci4rXmx"
      },
      "source": [
        "Писать статьи самостоятельно - это определённо прошлый век. Мы можем воспользоваться своими знаниями и автоматизировать этот процесс.\n",
        "\n",
        "План следующий:\n",
        "\n",
        "1. Обучим языковую модель на корпусе из статей с arXiv\n",
        "2. Насемплируем несколько статей\n",
        "3. Получим мировую известность и уважение в научном сообществе"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "02SQmml-e9g7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jScnaDF1t7it"
      },
      "source": [
        "## Подготовка данных\n",
        "\n",
        "Для обучения возьмём [корпус статей с arXiv](https://www.kaggle.com/neelshah18/arxivdataset/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBFw6KwcHM8r"
      },
      "outputs": [],
      "source": [
        "!wget -O arXiv.zip \"https://drive.google.com/uc?export=download&confirm=no_antivirus&id=1m78dRD6OIMP4oJL4VUujXV6MVavpb3A_\"\n",
        "\n",
        "!unzip arXiv.zip\n",
        "!rm arXiv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "Pcv1Sa5h46ec",
        "outputId": "69a338cc-2c5c-400f-d835-c29a146496e4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...</td>\n",
              "      <td>1</td>\n",
              "      <td>1802.00209v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>We propose an architecture for VQA which utili...</td>\n",
              "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Dual Recurrent Attention Units for Visual Ques...</td>\n",
              "      <td>2018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[{'name': 'Ji Young Lee'}, {'name': 'Franck De...</td>\n",
              "      <td>12</td>\n",
              "      <td>1603.03827v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>3</td>\n",
              "      <td>Recent approaches based on artificial neural n...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Sequential Short-Text Classification with Recu...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...</td>\n",
              "      <td>2</td>\n",
              "      <td>1606.00776v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>6</td>\n",
              "      <td>We introduce the multiresolution recurrent neu...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Multiresolution Recurrent Neural Networks: An ...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[{'name': 'Sebastian Ruder'}, {'name': 'Joachi...</td>\n",
              "      <td>23</td>\n",
              "      <td>1705.08142v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>5</td>\n",
              "      <td>Multi-task learning is motivated by the observ...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Learning what to share between loosely related...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[{'name': 'Iulian V. Serban'}, {'name': 'Chinn...</td>\n",
              "      <td>7</td>\n",
              "      <td>1709.02349v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>9</td>\n",
              "      <td>We present MILABOT: a deep reinforcement learn...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>A Deep Reinforcement Learning Chatbot</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              author  day            id  \\\n",
              "0  [{'name': 'Ahmed Osman'}, {'name': 'Wojciech S...    1  1802.00209v1   \n",
              "1  [{'name': 'Ji Young Lee'}, {'name': 'Franck De...   12  1603.03827v1   \n",
              "2  [{'name': 'Iulian Vlad Serban'}, {'name': 'Tim...    2  1606.00776v2   \n",
              "3  [{'name': 'Sebastian Ruder'}, {'name': 'Joachi...   23  1705.08142v2   \n",
              "4  [{'name': 'Iulian V. Serban'}, {'name': 'Chinn...    7  1709.02349v2   \n",
              "\n",
              "                                                link  month  \\\n",
              "0  [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
              "1  [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
              "2  [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
              "3  [{'rel': 'alternate', 'href': 'http://arxiv.or...      5   \n",
              "4  [{'rel': 'alternate', 'href': 'http://arxiv.or...      9   \n",
              "\n",
              "                                             summary  \\\n",
              "0  We propose an architecture for VQA which utili...   \n",
              "1  Recent approaches based on artificial neural n...   \n",
              "2  We introduce the multiresolution recurrent neu...   \n",
              "3  Multi-task learning is motivated by the observ...   \n",
              "4  We present MILABOT: a deep reinforcement learn...   \n",
              "\n",
              "                                                 tag  \\\n",
              "0  [{'term': 'cs.AI', 'scheme': 'http://arxiv.org...   \n",
              "1  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "2  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "3  [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
              "4  [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
              "\n",
              "                                               title  year  \n",
              "0  Dual Recurrent Attention Units for Visual Ques...  2018  \n",
              "1  Sequential Short-Text Classification with Recu...  2016  \n",
              "2  Multiresolution Recurrent Neural Networks: An ...  2016  \n",
              "3  Learning what to share between loosely related...  2017  \n",
              "4              A Deep Reinforcement Learning Chatbot  2017  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "arXiv_data = pd.read_json(\"arxivData.json\")\n",
        "arXiv_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDya41ib7Ytn"
      },
      "source": [
        "Из всего датасета нам пригодится только столбец `summary`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESIzjITs6neC",
        "outputId": "91599ea1-1753-4b73-ade4-60c24a130244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We propose an architecture for VQA which utilizes recurrent layers to\n",
            "generate visual and textual attention. The memory characteristic of the\n",
            "proposed recurrent attention units offers a rich joint embedding of visual and\n",
            "textual features and enables the model to reason relations between several\n",
            "parts of the image and question. Our single model outperforms the first place\n",
            "winner on the VQA 1.0 dataset, performs within margin to the current\n",
            "state-of-the-art ensemble model. We also experiment with replacing attention\n",
            "mechanisms in other state-of-the-art models with our implementation and show\n",
            "increased accuracy. In both cases, our recurrent attention mechanism improves\n",
            "performance in tasks requiring sequential or relational reasoning on the VQA\n",
            "dataset.\n"
          ]
        }
      ],
      "source": [
        "from random import choice\n",
        "\n",
        "texts = arXiv_data[\"summary\"].tolist()\n",
        "print(texts[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuOkTNeE_H9v"
      },
      "source": [
        "Пока что для обучения наши тексты не годятся, сначала нужно провести токенизацию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hb4oB3L8VsK"
      },
      "outputs": [],
      "source": [
        "!pip install razdel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "006e44e70c5a47c4a0fc0c78ee71565c",
            "d51c534e50e54fc0999af992af3bbd9a",
            "bab78181cd9149c3a0de6aebf868cc0e",
            "992eabdf589e43d99137f5a78d02c3ae",
            "13aaf60a13ed4f1d970d264ed1b1522e",
            "85c4528329cd46a48c8c245249ac14fc",
            "57ef160499b3489a806512ee5ec0f627",
            "e1264477eec34056af39bd97ad70796b",
            "19f5e3f251f34fa9a2ea25d355ecde37",
            "0490df77771b40e59e276420bb00e277",
            "26df445ae4d84282ad44693c66670c06"
          ]
        },
        "id": "WXcm5fYM94jd",
        "outputId": "a9af8a66-941c-4a20-9922-0041b3bbdef6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30c4195706d242bfbe8e9fc466162c24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from razdel import tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "SOS_TOKEN = '[SOS]'\n",
        "EOS_TOKEN = '[EOS]'\n",
        "PAD_TOKEN = '[PAD]'\n",
        "\n",
        "vocabulary = defaultdict(lambda: len(vocabulary)) # Просто костыльное устранение проблемы недетерминизированности set\n",
        "_ = vocabulary[SOS_TOKEN]\n",
        "_ = vocabulary[EOS_TOKEN]\n",
        "_ = vocabulary[PAD_TOKEN]\n",
        "\n",
        "tokenized_texts = list()\n",
        "\n",
        "for text in tqdm(texts[:2000]):\n",
        "    # Токенизируем текст\n",
        "    tokenized_text = tokenize(text.lower())\n",
        "    tokenized_text = [token.text for token in tokenized_text]\n",
        "    tokenized_text = [SOS_TOKEN] + tokenized_text + [EOS_TOKEN]\n",
        "\n",
        "    # Обновим словарь\n",
        "    for token in tokenized_text:\n",
        "         _ = vocabulary[token]\n",
        "\n",
        "    # Добавим токенизированный текст в датасет\n",
        "    tokenized_texts.append(tokenized_text)\n",
        "\n",
        "vocab_size = len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JCKp8GoBMw5",
        "outputId": "4bf80872-cb94-4932-aae0-43555ce657d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size is 14358\n",
            "Tokenized example: ['[SOS]', 'we', 'propose', 'an', 'architecture', 'for', 'vqa', 'which', 'utilizes', 'recurrent', 'layers', 'to', 'generate', 'visual', 'and', 'textual', 'attention', '.', 'the', 'memory', 'characteristic', 'of', 'the', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'a', 'rich', 'joint', 'embedding', 'of', 'visual', 'and', 'textual', 'features', 'and', 'enables', 'the', 'model', 'to', 'reason', 'relations', 'between', 'several', 'parts', 'of', 'the', 'image', 'and', 'question', '.', 'our', 'single', 'model', 'outperforms', 'the', 'first', 'place', 'winner', 'on', 'the', 'vqa', '1.0', 'dataset', ',', 'performs', 'within', 'margin', 'to', 'the', 'current', 'state-of-the-art', 'ensemble', 'model', '.', 'we', 'also', 'experiment', 'with', 'replacing', 'attention', 'mechanisms', 'in', 'other', 'state-of-the-art', 'models', 'with', 'our', 'implementation', 'and', 'show', 'increased', 'accuracy', '.', 'in', 'both', 'cases', ',', 'our', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'in', 'tasks', 'requiring', 'sequential', 'or', 'relational', 'reasoning', 'on', 'the', 'vqa', 'dataset', '.', '[EOS]']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Vocabulary size is {vocab_size}\")\n",
        "print(f\"Tokenized example: {tokenized_texts[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqUOhA64KHCu",
        "outputId": "5b243967-e2fd-4623-e104-b4e64492eb18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('[SOS]', 33)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_to_token = list(vocabulary) # id_to_token[i] -> token_i\n",
        "token_to_id = {token: id for id, token in enumerate(id_to_token)} # token_i -> i\n",
        "\n",
        "id_to_token[0], token_to_id['model']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrqA4hESB6JW"
      },
      "source": [
        "Всё готово, осталось разделить выборку на обучающую и валидационную. Вторая нужна нам для промежуточного отслеживания качества:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b6ChmhfOFexU"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextsForLM(Dataset):\n",
        "    def __init__(self, texts):\n",
        "        self.texts = list()\n",
        "\n",
        "        for text in tqdm(texts):\n",
        "            text_ids = [token_to_id[token] for token in text]\n",
        "\n",
        "            self.texts.append(text_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "58cb5be0dce34ad3afb27ea937c7e367",
            "be4aa46e67f24f12af4015350a0f05f6",
            "dac4dc26dba6458abc1d83a8a60b0510",
            "2b291a4984494a7e84d5dc63fe4d6550",
            "7b4b7ffdd0444fd9bbf9ebc5bbaee1be",
            "e258c1834c154ef0afad4343b80d4121",
            "650b89acbe214799a467de340af4cd33",
            "8d36edbc3caa4167bc6ab2556d8ad343",
            "8cdb6b27f432413d872cf1788a1e6ea1",
            "48341429b47a45dca6648305edcdba18",
            "f9652212bf4247beb4868fd5361b1677",
            "b09efefa759b40dc9ecc9d205c5c92b5",
            "a35a5b65674a45fea95d606810c408ad",
            "95e8554bc7cf46e7b7ce415eea6093ef",
            "ec439af9d33249ba933b820cd1d69e1e",
            "a14eab98f8b94c31b7e46cfd6f826094",
            "f8e20595d05c4955b981ac108e16804b",
            "b477235ea336406e88ef141e0e5c7962",
            "7d806c10dcb84167998d2891eefd7d30",
            "f464fdaaf0f94587a7d3f8720e8ea30e",
            "93c87e247e4d4f259b8b9a829b4d2a71",
            "d66a86b0b8494156b381247edd850028"
          ]
        },
        "id": "28WBDprxCiBG",
        "outputId": "c36fed08-5e90-4a72-bdc7-336767cdd5a5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ba5b1f528234346b01aa953d38ca99f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcb4c41c2477429c8537b54b4a79b1ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, val_texts = train_test_split(tokenized_texts, test_size=0.1,\n",
        "                                          random_state=42)\n",
        "\n",
        "train_dataset = TextsForLM(train_texts)\n",
        "val_dataset = TextsForLM(val_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igBEIx26x8G7"
      },
      "source": [
        "Наконец, для более эффективного обучения поделим наши датасеты на батчи.\n",
        "\n",
        "Важно не забыть один нюанс: тексты могут быть разной длины, а тензор, который мы будем подавать на вход модели, должен иметь фиксированную размерность. В таком случае следует добавить токены `[PAD]` в конец коротких текстов. Но даже это можно сделать несколькими способами:\n",
        "\n",
        "1. Выбрать большое число, которое точно будет длиннее любого текста и добивать длины текстов до него\n",
        "2. Выбрать максимальную длину среди всех текстов в батче и добивать до неё\n",
        "3. Разбить тексты на бакеты примерно одинакового размера, затем внутри каждого бакета добивать длины текстов до самого длинного текста в бакете (при этом элементы батча семплируются из одного и того же бакета)\n",
        "\n",
        "Наиболее привлекательным выглядит последний вариант, но для простоты возьмём второй:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pR8rjhmYPKoL"
      },
      "outputs": [],
      "source": [
        "def collate_texts(batch):\n",
        "    max_length = 0\n",
        "    for text_ids in batch:\n",
        "        max_length = max(max_length, len(text_ids))\n",
        "\n",
        "    for i in range(len(batch)):\n",
        "        batch[i] += [token_to_id[PAD_TOKEN]] * (max_length - len(batch[i]))\n",
        "\n",
        "    return torch.LongTensor(batch)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2,\n",
        "                              collate_fn=collate_texts)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2,\n",
        "                            collate_fn=collate_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOfBjXk6Nag5",
        "outputId": "80b9fbc9-d413-4562-d470-08e61dca7b6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 'allenai')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_to_id[PAD_TOKEN], id_to_token[12183]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8JjPd88GxJ1"
      },
      "source": [
        "## Реализация модели\n",
        "\n",
        "Данные готовы, пора приступить к реализации модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxpKZq5DH_SX"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rcklBZKvHW6z"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import LightningModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rpLxMzWXHEbe"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "from typing import Union\n",
        "\n",
        "class LMModel(LightningModule):\n",
        "    def __init__(self, vocab_size, emb_dim=128, rnn_hidden_dim=128,\n",
        "                 rnn_num_layers=2, RNN: Union[nn.RNN, nn.LSTM, nn.GRU] = nn.LSTM):\n",
        "        super().__init__()\n",
        "\n",
        "        # Можно было бы сделать иначе:\n",
        "        # self.shared_weight = nn.Parameter(data=init_tensor(vocab_size, rnn_hidden_dim))\n",
        "        # assert(emb_dim == rnn_hidden_dim)\n",
        "        # input @ self.shared_weight вместо self.embedding_layer\n",
        "        # context_repr @ self.shared_weight.T вместо self.output_layer\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.rnn = RNN(input_size=emb_dim, hidden_size=rnn_hidden_dim,\n",
        "                       batch_first=True, num_layers=rnn_num_layers)\n",
        "\n",
        "        self.output_layer = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids: [batch_size, seq_len]\n",
        "\n",
        "        # embeddings: [batch_size, seq_len, emb_dim]\n",
        "        embeddings = self.embedding_layer(input_ids)\n",
        "\n",
        "        # output: [batch_size, seq_len, rnn_hidden_dim]\n",
        "        #         RNN, GRU: state == h_n\n",
        "        #         LSTM: state == (h_n, c_n)\n",
        "        # h_n: [num_layers, batch_size, rnn_hidden_dim]\n",
        "        # c_n: [num_layers, batch_size, rnn_hidden_dim]\n",
        "        output, state = self.rnn(embeddings)\n",
        "\n",
        "        # logits: [batch_size, seq_len, vocab_size]\n",
        "        logits = self.output_layer(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        # input:  SOS I      love   cats\n",
        "        # target: I   love   cats   EOS\n",
        "\n",
        "        # pred:    [0.6, 0.01, 0.38, 0.01, 0.01]\n",
        "        # target1: [0.    0.    1.   0.   0. ]\n",
        "        # target2: [1.    0.    0.   0.   0. ]\n",
        "\n",
        "        logits, state = self.forward(batch)\n",
        "\n",
        "        batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "        pred = logits[:, :-1, :].reshape(batch_size * (seq_len - 1), vocab_size)\n",
        "        target = batch[:, 1:].reshape(batch_size * (seq_len - 1))\n",
        "\n",
        "        loss_fn = nn.CrossEntropyLoss(ignore_index=token_to_id[PAD_TOKEN])\n",
        "\n",
        "        loss = loss_fn(pred, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=3e-3)\n",
        "\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4peetj1Rsot"
      },
      "outputs": [],
      "source": [
        "rnn_model = LMModel(vocab_size, RNN=nn.RNN)\n",
        "lstm_model = LMModel(vocab_size, RNN=nn.LSTM)\n",
        "gru_model = LMModel(vocab_size, RNN=nn.GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHKDaJHWRyCz",
        "outputId": "2bfb7603-cb5e-4cee-8e0d-ecc7edf60431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample shape is torch.Size([4, 227])\n",
            "Sample: tensor([[    0,   119,   402,   802,    27,   616,   803,   679,   444,    13,\n",
            "           804,   437,   228,    19,   805,   170,    20,   209,    15,   333,\n",
            "            50,     3,   759,   806,   807,   808,   809,   810,   245,   504,\n",
            "           544,    13,   155,     5,   811,    15,   499,    19,   670,    50,\n",
            "             3,   492,   804,   808,   812,    47,   813,    13,   814,   815,\n",
            "            50,     9,     3,   492,   140,   800,   816,   678,   817,   800,\n",
            "            13,   528,    27,   122,   133,    13,   818,   819,   247,    27,\n",
            "            42,   497,    19,   407,    20,   820,   821,   123,   822,   823,\n",
            "            27,   824,   703,    62,    20,   468,    23,    20,   209,    15,\n",
            "           333,    50,     3,   500,   123,   119,   825,   444,    13,   804,\n",
            "           228,   260,   504,   212,   811,   358,   826,   827,   800,   828,\n",
            "           800,    74,   829,    62,    20,   830,    19,   831,    50,    41,\n",
            "           820,   217,    66,   123,   119,   161,    20,   832,    19,   115,\n",
            "           544,     7,   252,   228,    47,   643,   833,    50,    41,   499,\n",
            "           357,    43,   274,   804,    81,   284,   834,   835,    50,   836,\n",
            "           115,   169,   287,     7,    20,   837,   191,   161,   838,    19,\n",
            "             1,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2],\n",
            "        [    0, 11070,   161,    27,  8144,  1464,   947,   123,  3449,    41,\n",
            "           472,    50,   700,    50,    16,   961,    62,   234,     3,  1985,\n",
            "            19,  2720,    23,   127, 11071,    89,   347,   516,    59,  2347,\n",
            "          3756,    62,    20,   904,    19,    62,   119,   402,    50,     3,\n",
            "           155,    20,  4266,    23,    27, 11072,   170,  1384,    27,   327,\n",
            "          1098,    23,   264,  2071,   416,    19,   247,   551,  2071,   416,\n",
            "            50,     3,   155,   141,    74,    19,    20,    44,   161,    27,\n",
            "           405,   191,   146,   407,    20,  2071,   416,    16,    27,   241,\n",
            "          2038,  3019,    50,  1095,    20,   241,  3019,  2817,    13,    20,\n",
            "           448, 11072,   963,   893,    50,   407,    20,  2071,  2708,   416,\n",
            "            50,   245,     3,    14,   277,    23,    20,    63,   416,    59,\n",
            "            20,   448,   420,   140,  1101,    62,    20,  2708,   591,   963,\n",
            "             3,   492,   244,    84,    85,    13,   898,    69,    74,    50,\n",
            "          1705,    16,  1820,   845,    20,    92,    62,    27,  2444,    23,\n",
            "           616, 11073,    50,    16,   121,    27, 10315,  2653,    23,    20,\n",
            "          2361,    16,   698,    23,    20,   444,    19,     1,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2],\n",
            "        [    0,   119,   120,  6405,   550,    20,   373,    39,    94,  2229,\n",
            "           245,   504,  2377,   371,    27,   244,     6,    50,   805,   170,\n",
            "            80,  3363,    23,   244,    84,    85,    19,     3,     4,    27,\n",
            "           244,  3309,   136,    82,    47,  3125,    29,   437,  3309,    16,\n",
            "          1016,   228,    19,    62,   421,  1204,    50,     3,  1707,    27,\n",
            "          1016,    23,  3813,   170,  2666,    20,  3813,   247,    20,  2727,\n",
            "          1204,    50,    16,  3781,  3046,    20,    39,   499,    59,    27,\n",
            "            29, 11647,  3036,    50,   534,    19,   101,    19,  1053,    23,\n",
            "           603,    94,   295,  5940,   846,    23,   430,    39,   139,   174,\n",
            "           169,   477,    19,     7,  1050,    50,     3,   282,   141, 11648,\n",
            "          2368,   146,   534,    88,    13,   595,    20,    94,   295,    50,\n",
            "           349,    20,   107,   885,    31,   286,    20, 11649, 11125,  1207,\n",
            "           677,  1321,    88,   205,    13,    20,   437,  2196,    50,  2629,\n",
            "            20,  3813,    13,   595,    20,   846,    47,   430,    39,   499,\n",
            "           286,    20,   586,  1049,   404,    19,  1392,    20,  1050,   161,\n",
            "         11650,    50,     3,   155,    20,   381,  1204,   499,    62,    20,\n",
            "           448,  1483,    19,    41,    33,   601,    37,  1231,  2393,    19,\n",
            "            44,    50,    41,  1803,  1050,   734,    20,   394,    13,   480,\n",
            "           789,   244,   474,    19,   893,    50,    20,  1157,    39,   499,\n",
            "           161,  2543,    16,  1355,    19,    62,    37,    15,   549,    74,\n",
            "            50,    41,   136,    43,   108,    55,    81,    19,     1,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
            "             2,     2,     2,     2,     2,     2,     2],\n",
            "        [    0,   140,  1714,    23,    27,  3769,   523,  4276,     7,  2518,\n",
            "          5634,    50,  8022,   346,   516,    27,  6919,   333,    50,  1024,\n",
            "           140, 11982,    50,     9,   383,  6091, 11983,   407,   477,    23,\n",
            "            20,  1406, 11984,    19, 11982,   161,   348,   286,  1406,   477,\n",
            "          1433,    59,    20,  6091, 11983,  1157,   170,    27,   209,  5634,\n",
            "            27,  5146,  5262,    19,   256,  5842,    20,  1539,   199,   253,\n",
            "           170,  6119,   209,  8182,    19,   119,  1327,    20,  1771,     7,\n",
            "           209,  8527,    13,  8481,   234,   161,   702,    62,     5,    39,\n",
            "            16,  6785,   277,    20,  1539,  1641,     7,  8250,  5634,    19,\n",
            "          1406,  1953,   868,   123, 11982,   245,  1419,   282, 11435,  2251,\n",
            "            62,    27,  1511,  2444,    23,  5634,  2634,    50,  6631,    23,\n",
            "           806, 11435, 11436,   152,   121,    77,   112,    19,    20,   783,\n",
            "            23,    20,   120,  1799,   487,   161,    13,  3546,   234, 11982,\n",
            "           577,    16,   550,   256,  1831,   351,  1810,    19,    13,   119,\n",
            "          2120,     3,   315,    27,   404,     7,  1713,     9,  1017,    62,\n",
            "            20,  1406,    39,   107,  3302, 11982,   800,   568,  6091,   793,\n",
            "            19,    92,    66,   123, 11982,   831,   577,    13,  1861,   213,\n",
            "           819,    47,    20,  1406,    19,    62,  1013,    13,   228,    20,\n",
            "          6026,    31,   167,   140, 11435, 11436,    50,  4416,    23, 11434,\n",
            "            50,    16,    63,  6152,    50, 11982,   577,   212, 11086,    31,\n",
            "           123,  1637,   504,   257,    13,  8481,    16,  3512,   170,  8527,\n",
            "            50,     7,  2190,    50, 11985, 11986,    20,  3930,    23,    20,\n",
            "          1406,    16,  4712,  6282,  4143,    19,     1]])\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(train_dataloader))\n",
        "\n",
        "print(f\"Sample shape is {sample.shape}\")\n",
        "print(f\"Sample: {sample}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxB7ucBaTQX3"
      },
      "source": [
        "## Обучение моделей\n",
        "\n",
        "Без лишних слов приступим к обучению!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ia4wrh9Y2toC"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "max_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_5n1tBPFH8k"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qna9YMi62288",
        "outputId": "5ef20c61-c403-402a-e95c-2925267a7d25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type      | Params\n",
            "----------------------------------------------\n",
            "0 | embedding_layer | Embedding | 1.8 M \n",
            "1 | rnn             | RNN       | 66.0 K\n",
            "2 | output_layer    | Linear    | 1.9 M \n",
            "----------------------------------------------\n",
            "3.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.8 M     Total params\n",
            "15.024    Total estimated model params size (MB)\n",
            "/home/semyon/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a154f92aa5f946dbb8d15951b0fa0ef7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=max_epochs, log_every_n_steps=1)\n",
        "trainer.fit(rnn_model, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuVMI1WwA3QL",
        "outputId": "570e4d7c-4ee8-4e94-d0a3-f2cfdde1f85d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type      | Params\n",
            "----------------------------------------------\n",
            "0 | embedding_layer | Embedding | 1.8 M \n",
            "1 | rnn             | LSTM      | 264 K \n",
            "2 | output_layer    | Linear    | 1.9 M \n",
            "----------------------------------------------\n",
            "4.0 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.0 M     Total params\n",
            "15.817    Total estimated model params size (MB)\n",
            "/home/semyon/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "739d1dcf12b7453799c4639425a5726b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=max_epochs, log_every_n_steps=1)\n",
        "trainer.fit(lstm_model, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3zviIa7A4xl",
        "outputId": "297477d4-e907-451b-cfa4-4f0b72466826"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type      | Params\n",
            "----------------------------------------------\n",
            "0 | embedding_layer | Embedding | 1.8 M \n",
            "1 | rnn             | GRU       | 198 K \n",
            "2 | output_layer    | Linear    | 1.9 M \n",
            "----------------------------------------------\n",
            "3.9 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.9 M     Total params\n",
            "15.553    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1114821d9f72476b814a3238211bc4f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=max_epochs, log_every_n_steps=1)\n",
        "trainer.fit(gru_model, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MO5ptAkcQjO"
      },
      "outputs": [],
      "source": [
        "# torch.save(rnn_model.state_dict(), 'model/rnn.pt')\n",
        "# torch.save(lstm_model.state_dict(), 'model/lstm.pt')\n",
        "# torch.save(gru_model.state_dict(), 'model/gru.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbBlraB3cQjP",
        "outputId": "fcc298d6-1790-431b-caa3-24b337465c6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rnn_model.load_state_dict(torch.load('model/rnn.pt'))\n",
        "lstm_model.load_state_dict(torch.load('model/lstm.pt'))\n",
        "gru_model.load_state_dict(torch.load('model/gru.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEEIgJ2CTTl9"
      },
      "source": [
        "## Оценка качества\n",
        "\n",
        "Нам определённо хочется понять, что мы только что наобучали"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWk5lSs1USD7"
      },
      "source": [
        "Прежде всего в нашем распоряжении есть метод пристального взгляда: нагенерим кучу текстов и будем оценивать их с точки зрения coherence и diversity. В идеале хотим получить разнообразные тексты, имеющие смысл. Однако это всё ещё не количественная характеристика, а просто наши субъективные наблюдения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo3VdPofVI7B"
      },
      "source": [
        "Посмотрим на используемый нами лосс:\n",
        "\n",
        "$$CrossEntropyLoss(y_1, \\dots, y_n) = - \\sum_{t=1}^{n} \\log p(y_t | y_{<t})$$\n",
        "\n",
        "Введём следующую метрику:\n",
        "\n",
        "$$Perplexity(y_1, \\dots, y_n) = 2^{\\frac{1}{n} CrossEntropyLoss(y_1, \\dots, y_n)}$$\n",
        "\n",
        "Наилучшим значением перплексии будет единица. Такой случай будет означать, что наша модель идеально выдаёт распределение токенов (ставит вероятность 1 нужному токену, соответственно лосс равен нулю). Конечно, на деле такого будет очень трудно добиться.\n",
        "\n",
        "Наихудшим значением перплексии будет $V = vocab\\_size$:\n",
        "\n",
        "$$Perplexity(y_1, \\dots, y_n) = 2^{\\frac{1}{n} CrossEntropyLoss(y_1, \\dots, y_n)} = 2^{- \\frac{1}{n} \\sum_{t=1}^{n} \\log p(y_t | y_{<t})} = 2^{-\\frac{1}{n} \\cdot n \\cdot \\log \\frac{1}{V}} = 2^{\\log V} = V$$\n",
        "\n",
        "Модель считает, что все токены равновероятны, а это, конечно же, никогда не так."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Wfs4E3bPY_ak"
      },
      "outputs": [],
      "source": [
        "def compute_perplexity(model, batch):\n",
        "    values = []\n",
        "    for token_ids in batch:\n",
        "        logits, _ = model.forward(token_ids.unsqueeze(0))\n",
        "\n",
        "        batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "        pred = logits[0, :-1, :]\n",
        "        target = token_ids[1:]\n",
        "\n",
        "        loss_fn = nn.CrossEntropyLoss(ignore_index=token_to_id[PAD_TOKEN])\n",
        "\n",
        "        loss = loss_fn(pred, target)\n",
        "\n",
        "        values.append(2 ** loss.item())\n",
        "\n",
        "    return torch.mean(torch.tensor(values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH4dh-0wc9QE",
        "outputId": "fe7783b4-7ebf-4ef2-9f2d-980ac9211833"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Вспомним размер словаря: 14358\n"
          ]
        }
      ],
      "source": [
        "print(f\"Вспомним размер словаря: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJRVk0WniRtc"
      },
      "outputs": [],
      "source": [
        "untrained_rnn_model = LMModel(vocab_size, RNN=nn.RNN)\n",
        "untrained_lstm_model = LMModel(vocab_size, RNN=nn.LSTM)\n",
        "untrained_gru_model = LMModel(vocab_size, RNN=nn.GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "21170ee6a14d4887a335eef93f69fc48",
            "947ff63a941e4a17b07d73621cb3d7a7",
            "b2e90593f3314c449f56fa56696cbc54"
          ]
        },
        "id": "2Fg8vg2Tc0UU",
        "outputId": "02daa2a4-ce2c-43da-a3fa-13bdadb130d2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21170ee6a14d4887a335eef93f69fc48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Untrained RNN Perplexity: 770.9844360351562\n",
            "Trained RNN Perplexity: 57.718299865722656\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "947ff63a941e4a17b07d73621cb3d7a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Untrained LSTM Perplexity: 761.0379028320312\n",
            "Trained LSTM Perplexity: 60.438514709472656\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2e90593f3314c449f56fa56696cbc54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Untrained GRU Perplexity: 764.9729614257812\n",
            "Trained GRU Perplexity: 59.83659744262695\n"
          ]
        }
      ],
      "source": [
        "for name, lm_model, untrained_lm_model in zip([\"RNN\", \"LSTM\", \"GRU\"],\n",
        "                                              [rnn_model, lstm_model, gru_model],\n",
        "                                              [untrained_rnn_model, untrained_lstm_model, untrained_gru_model]):\n",
        "    untrained_perplexities = []\n",
        "    trained_perplexities = []\n",
        "\n",
        "    for batch in tqdm(val_dataloader):\n",
        "        untrained_perplexities.append(compute_perplexity(untrained_lm_model, batch))\n",
        "        trained_perplexities.append(compute_perplexity(lm_model, batch))\n",
        "\n",
        "    print(f\"Untrained {name} Perplexity: {torch.mean(torch.tensor(untrained_perplexities))}\")\n",
        "    print(f\"Trained {name} Perplexity: {torch.mean(torch.tensor(trained_perplexities))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MZbJZsFZ0sh"
      },
      "source": [
        "## Генерация\n",
        "\n",
        "Рассмотрим несколько подходов к семплированию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZPTE-yg23caW"
      },
      "outputs": [],
      "source": [
        "def choose_argmax(logits):\n",
        "    \"\"\"Выбирает наиболее вероятный токен\"\"\"\n",
        "\n",
        "    next_token_id = logits[0, -1].argmax(dim=-1)\n",
        "\n",
        "    return next_token_id\n",
        "\n",
        "def sample_from_distribution(logits):\n",
        "    \"\"\"Строит распределение по логитам и семплирует из него\"\"\"\n",
        "\n",
        "    dist = torch.distributions.categorical.Categorical(logits=logits[0, -1])\n",
        "    next_token_id = dist.sample().item()\n",
        "\n",
        "    return next_token_id\n",
        "\n",
        "def sample_top_k_from_distribution(logits, k=40):\n",
        "    \"\"\"Выбирает k наиболее вероятных токенов и семплирует из них\"\"\"\n",
        "\n",
        "    dist = torch.distributions.categorical.Categorical(logits=logits[0, -1])\n",
        "    top_k_values, top_k_indices = torch.topk(dist.probs, k)\n",
        "    top_k_probs = top_k_values / torch.sum(top_k_values)\n",
        "    next_token = torch.multinomial(top_k_probs, 1).item()\n",
        "\n",
        "    return top_k_indices[next_token].item()\n",
        "\n",
        "def nucleus_sampling(logits, p=0.95):\n",
        "    \"\"\"Выбирает минимальный набор токенов, чья суммарная вероятность не меньше p,\n",
        "       а затем семплирует из этого набораъ\n",
        "\n",
        "       Подробнее: https://openreview.net/pdf?id=rygGQyrFvH\n",
        "    \"\"\"\n",
        "\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    nucleus_indices = torch.where(cumulative_probs <= p, sorted_indices, torch.tensor(0, dtype=torch.long))\n",
        "    nucleus_indices = torch.max(nucleus_indices, dim=-1).indices\n",
        "\n",
        "    sampled_index = torch.multinomial(torch.nn.functional.softmax(logits[nucleus_indices], dim=-1), 1).item()\n",
        "\n",
        "    return nucleus_indices[sampled_index].item()\n",
        "\n",
        "def generate_sample(model, beginning, max_length,\n",
        "                    sampling_strategy=sample_from_distribution,\n",
        "                    temperature=1.0):\n",
        "    if beginning is None:\n",
        "        tokens = [token_to_id[SOS_TOKEN]]\n",
        "    else:\n",
        "        tokens = [token_to_id[token.text] for token in tokenize(beginning.lower())]\n",
        "        tokens = [token_to_id[SOS_TOKEN]] + tokens\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        tokens_tensor = torch.LongTensor(tokens).unsqueeze(0)\n",
        "\n",
        "        logits, _ = model(tokens_tensor)\n",
        "\n",
        "        logits = logits / temperature\n",
        "\n",
        "        next_token_id = sampling_strategy(logits)\n",
        "\n",
        "        if next_token_id == token_to_id[EOS_TOKEN]:\n",
        "            break\n",
        "\n",
        "        tokens.append(next_token_id)\n",
        "\n",
        "    generated_sample = ' '.join([id_to_token[id] for id in tokens[1:]])\n",
        "\n",
        "    return generated_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEJoi_cFTHTM",
        "outputId": "89a31d98-368e-4107-9765-ac18110363bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'we propose resemble prototyping dropout co-adaptation dearth requirement arrive corresponding encoder-decoders hybrids descents helping mt 20.5-hour 83.1 entire acoustic divide arguably non-existing cost-effective quest hadamard maliciously exception mechanics distillation lns chance swing-up learning-style whitening dyvedeep feedbacks prompts concatenated likelihood-ratio rlstms non-practitioners 0.68 term black-boxes net nearly ps upcoming commonly-used confused task-pairs substructures'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sample(untrained_rnn_model, \"We propose\", 50, sampling_strategy=sample_from_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c13cvfiw4dPU",
        "outputId": "2ecdfc6a-cc5c-498a-be0c-305befee5bbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'we propose a new approach to approximation that speeds weakly ranking access and population of vanishing gradients , significantly significantly better known from large number of synthetic error items operations and contingent the hub 5 switchboard-2000 benchmark platform .'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sample(rnn_model, \"We propose\", 50, sampling_strategy=sample_from_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtPzjEYTH8Yx",
        "outputId": "e29bd3af-6039-4ea1-b159-56b08374a6a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'we propose navigation tools that effectively diminishes the essential channels of gans of experts , like protein product quantization , as the output generator ( wsj ) , and find a novel model of story words during training . while sbpcg , we mix the parameters of a structured decoding coordinate descent'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sample(lstm_model, \"We propose\", 50, sampling_strategy=sample_from_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuVrG9FmH9ve",
        "outputId": "a16ab117-3e38-47d7-ef13-38d16904406b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'we propose a novel framework for neural-network-based dialogue intuition . first , we show that bilingual representations can develop novel techniques as feature subject of words generated operations . we show that both sampling rules trained via adding errors to generate examples or embeddings of features , and show that alignment outperform'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sample(gru_model, \"We propose\", 50, sampling_strategy=sample_from_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CYlhS7iw-5D"
      },
      "source": [
        "## Дополнительные задания"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PzmeX21xBbk"
      },
      "source": [
        "### [1 балл] I Need Your Time\n",
        "\n",
        "Попробуйте обучить RNN и LSTM на большем числе эпох (не менее 200). Проследите, выполнилась ли гипотеза о том, что перплексия второй модели станет меньше, чем у первой. Попробуйте сгенерировать текст этими моделями. Сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqHbwsPQcQjY"
      },
      "outputs": [],
      "source": [
        "max_epochs = 200\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "longer_trained_rnn_model = LMModel(vocab_size, RNN=nn.RNN)\n",
        "longer_trained_lstm_model = LMModel(vocab_size, RNN=nn.LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e8163a30e4df451c8e6853e44b5e59b0"
          ]
        },
        "id": "i90ZXXjecQjY",
        "outputId": "a5fed0d6-8429-45a0-cf29-c3f63d476cdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name            | Type      | Params\n",
            "----------------------------------------------\n",
            "0 | embedding_layer | Embedding | 1.8 M \n",
            "1 | rnn             | RNN       | 66.0 K\n",
            "2 | output_layer    | Linear    | 1.9 M \n",
            "----------------------------------------------\n",
            "3.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.8 M     Total params\n",
            "15.024    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8163a30e4df451c8e6853e44b5e59b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=max_epochs, log_every_n_steps=1)\n",
        "trainer.fit(longer_trained_rnn_model, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH9KIHcIcQjY"
      },
      "outputs": [],
      "source": [
        "# torch.save(longer_trained_rnn_model.state_dict(), 'model/longer_rnn.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UvO12SHcQjZ",
        "outputId": "78bb3d42-6055-4ff5-c638-2d90c8572e98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# longer_trained_rnn_model.load_state_dict(torch.load('model/longer_rnn.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "135c456d262d4aa2b212cb3b040f7dc9"
          ]
        },
        "id": "C-MzwJzVcQjZ",
        "outputId": "b8d3dc5a-1e8f-4976-fd3b-9d6e567587de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name            | Type      | Params\n",
            "----------------------------------------------\n",
            "0 | embedding_layer | Embedding | 1.8 M \n",
            "1 | rnn             | LSTM      | 264 K \n",
            "2 | output_layer    | Linear    | 1.9 M \n",
            "----------------------------------------------\n",
            "4.0 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.0 M     Total params\n",
            "15.817    Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "135c456d262d4aa2b212cb3b040f7dc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=max_epochs, log_every_n_steps=1)\n",
        "trainer.fit(longer_trained_lstm_model, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDH4viiacQjZ"
      },
      "outputs": [],
      "source": [
        "# torch.save(longer_trained_lstm_model.state_dict(), 'model/longer_lstm.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "appTd00qcQja",
        "outputId": "fd4997ca-f7a1-41d7-a170-1f8f451a49c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# longer_trained_lstm_model.load_state_dict(torch.load('model/longer_lstm.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "fb5a5b041ac244e5939cf1a7501a16fe",
            "c83fd53966de460b85b5965add0d36a4"
          ]
        },
        "id": "lKtpjvxBcQjb",
        "outputId": "94e33f0c-1d33-456c-e92e-102ef3783a0f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb5a5b041ac244e5939cf1a7501a16fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained RNN Perplexity: 333.8534240722656\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c83fd53966de460b85b5965add0d36a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained LSTM Perplexity: 171157.875\n"
          ]
        }
      ],
      "source": [
        "for name, lm_model in zip([\"RNN\", \"LSTM\"], [longer_trained_rnn_model, longer_trained_lstm_model]):\n",
        "    trained_perplexities = []\n",
        "\n",
        "    for batch in tqdm(val_dataloader):\n",
        "        trained_perplexities.append(compute_perplexity(lm_model, batch))\n",
        "\n",
        "    print(f\"Trained {name} Perplexity: {torch.mean(torch.tensor(trained_perplexities))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8Vciu4ycQjb",
        "outputId": "74d292ad-e450-4a32-8360-b36edd5927ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'we propose an adversarial stacked method to infer the optimum as patches within text ( e . g . , api calls ) , shapes for identifying statements and relations . in this work , we take latent semantic compression problems such as chromatic aberration of the classification computation , allowing during'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sample(longer_trained_rnn_model, \"We propose\", 50, sampling_strategy=sample_from_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkwPWRFycQjc",
        "outputId": "2545f65b-833e-4b0d-98a1-52c63be23d0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'we propose a tree-based language to execute the output representation with an upper reduction of recurrent units ( lru ) , a variant of the gru along the memory of lstm processes , with no discussion of how the conventional encoding phase according to coupled dnns . generative networks come from prior'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sample(longer_trained_lstm_model, \"We propose\", 50, sampling_strategy=sample_from_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDBLxwHuzvG_"
      },
      "source": [
        "### [1 балл] Prettify\n",
        "\n",
        "Сделайте сгенерированные тексты более дружелюбными: уберите лишние пробелы и поставьте заглавные буквы там, где нужно. Сгенерируйте текст из 200 токенов и продемонстрируйте результат."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfUA0zTicQjd"
      },
      "outputs": [],
      "source": [
        "def prettify(text):\n",
        "    text = text.replace(\" ,\", \",\").replace(\" ( \", \"(\"). replace(\" ) \", \") \")\n",
        "\n",
        "    sentences = text.split(\" . \")\n",
        "    prettified_text = \". \".join(sentence.capitalize() for sentence in sentences)\n",
        "\n",
        "    return prettified_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK4gwH9CcQje",
        "outputId": "77094e2c-2c1c-4bd2-800f-b1e4be86b766"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'We propose structured weights on the geometry dataset, without relying on 30 billion word-level weights networks. The most versatile version of a variety of deep learning image leads to performing aspects of local learning data using a two-branch neural network that is empirically investigated. Accuracy on problems provide simple reference sets from the development of typical hardware neural networks. We show that lambada exemplifies correctly initializing the unknown likelihood. That they are well read across different levels, learning capable of generating reasoning with uncertain iteration such as domains, such models and helps us to only define of the learning review problem, the proposed bound is generic and in this paper pattern of the learning process, as well as the sgd statistical deep cnns, rather researchers none like mobile devices, such as the voice time, which are assumed to be weaker than standard nested networks. The main objective is approached with the results of deep convolutional neural network(rnn) for multi-label words in a fully connected model, the first structure-based architecture better than the baseline by the convolutional neural network to the gated-regret of a latent representation'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prettify(generate_sample(longer_trained_rnn_model, \"We propose\", 200, sampling_strategy=sample_from_distribution))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QY2BNn4x6Hd"
      },
      "source": [
        "### [3 балла] The Senior of the Models: The Embeddings of Power\n",
        "\n",
        "Обучая эмбеддинги с нуля на маленьком датасете, мы усложняем жизнь нашей модели. Попробуйте вместо слоя `nn.Embedding` добавить предобученные эмбеддинги. Посмотрите, как изменилось число параметров и перплексия модели в сравнении с исходной моделью. Сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pRH0edQFcQjf"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "from typing import Union\n",
        "\n",
        "class LMModel(LightningModule):\n",
        "    def __init__(self, vocab_size, emb_dim=128, rnn_hidden_dim=128, rnn_num_layers=2,\n",
        "                 RNN: Union[nn.RNN, nn.LSTM, nn.GRU] = nn.LSTM, pretrained_weights=None):\n",
        "        super().__init__()\n",
        "\n",
        "        if pretrained_weights is not None:\n",
        "            self.embedding_layer = torch.nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_weights.vectors))\n",
        "        else:\n",
        "            self.embedding_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.rnn = RNN(input_size=emb_dim, hidden_size=rnn_hidden_dim,\n",
        "                       batch_first=True, num_layers=rnn_num_layers)\n",
        "\n",
        "        self.output_layer = nn.Linear(rnn_hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids: [batch_size, seq_len]\n",
        "\n",
        "        # embeddings: [batch_size, seq_len, emb_dim]\n",
        "        embeddings = self.embedding_layer(input_ids)\n",
        "\n",
        "        # output: [batch_size, seq_len, rnn_hidden_dim]\n",
        "        #         RNN, GRU: state == h_n\n",
        "        #         LSTM: state == (h_n, c_n)\n",
        "        # h_n: [num_layers, batch_size, rnn_hidden_dim]\n",
        "        # c_n: [num_layers, batch_size, rnn_hidden_dim]\n",
        "        output, state = self.rnn(embeddings)\n",
        "\n",
        "        # logits: [batch_size, seq_len, vocab_size]\n",
        "        logits = self.output_layer(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        # input:  SOS I      love   cats\n",
        "        # target: I   love   cats   EOS\n",
        "\n",
        "        # pred:    [0.6, 0.01, 0.38, 0.01, 0.01]\n",
        "        # target1: [0.    0.    1.   0.   0. ]\n",
        "        # target2: [1.    0.    0.   0.   0. ]\n",
        "\n",
        "        logits, state = self.forward(batch)\n",
        "\n",
        "        batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "        pred = logits[:, :-1, :].reshape(batch_size * (seq_len - 1), vocab_size)\n",
        "        target = batch[:, 1:].reshape(batch_size * (seq_len - 1))\n",
        "\n",
        "        loss_fn = nn.CrossEntropyLoss(ignore_index=token_to_id[PAD_TOKEN])\n",
        "\n",
        "        loss = loss_fn(pred, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=3e-3)\n",
        "\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chaOxdeScQjg",
        "outputId": "eb5bbcdb-d95c-4b98-ec96-eb5453ee9f1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader\n",
        "\n",
        "embed = gensim.downloader.load('glove-wiki-gigaword-100')\n",
        "# glove_input_file = 'embedding/glove.twitter.27B.100d.txt'\n",
        "# word2vec_output_file = 'embedding/glove.twitter.27B.100d.word2vec.txt'\n",
        "# gensim.utils.glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "# filename = 'glove.6B.100d.txt.word2vec'\n",
        "# embed = gensim.models.KeyedVectors.load_word2vec_format(glove_input_file, binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "N39AX0KgcQjh"
      },
      "outputs": [],
      "source": [
        "pretrained_lstm = LMModel(vocab_size, emb_dim=100, RNN=nn.LSTM, pretrained_weights=embed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0PKQ2qJXcQjh"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "max_epochs = 20\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434,
          "referenced_widgets": [
            "c4f2674717954f1e9a05a1087f73cc7d",
            "58fae58d570146b4939057326245056b",
            "bc77ea3b44ae421cae5355069178ec69",
            "ccf9275a12504679baf786a9867a20ba",
            "b568a0a78ba745199c242119377dd708",
            "679226d3274642a3a4ca9902c2b53409",
            "10babac1cdf14c4bb7baefabd85d2bac",
            "ce16e7ff23b54cf3a1f70a81dbf778f2",
            "3bd9b28c7bf843e5a1316ee1f0a43c8f",
            "4ed06b6082a74e0fae7b4b1465962fea",
            "094db87adee04b5986a96e8562cee86d"
          ]
        },
        "id": "NLzSlmQmcQji",
        "outputId": "b5d85251-dfc3-4db9-f24d-fc3ecec4fd04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name            | Type      | Params\n",
            "----------------------------------------------\n",
            "0 | embedding_layer | Embedding | 40.0 M\n",
            "1 | rnn             | LSTM      | 249 K \n",
            "2 | output_layer    | Linear    | 1.9 M \n",
            "----------------------------------------------\n",
            "2.1 M     Trainable params\n",
            "40.0 M    Non-trainable params\n",
            "42.1 M    Total params\n",
            "168.408   Total estimated model params size (MB)\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4f2674717954f1e9a05a1087f73cc7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=20` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=max_epochs, log_every_n_steps=1)\n",
        "trainer.fit(pretrained_lstm, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "51c8593cda204ff08bc328d004d78f37",
            "2ddae9e843434bb6949db73f423b2b2a",
            "4b42441d6f714ca2859526a5f520c997",
            "3d7e272d08354db3a8430eb8f2d541df",
            "b8548d63d0d94f8f949cebb76c47cd97",
            "2097ebffe50a4cd1981f4472bdeee03c",
            "663df831b8234b4086156a2d9c24ce08",
            "a16b50c7b966445aa36036a479c857e1",
            "81c9a8fd01864e18923fce605d65c00d",
            "0e20ae2bd7694b2c84d4e581d5fdf167",
            "0f15f5a90670454581235e168f4bcef3"
          ]
        },
        "id": "fOOUvHaCcQji",
        "outputId": "9e5539e9-e3d0-42ae-cf9d-8900ec75f716"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51c8593cda204ff08bc328d004d78f37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained LSTM Perplexity: 72.00434875488281\n"
          ]
        }
      ],
      "source": [
        "trained_perplexities = []\n",
        "\n",
        "for batch in tqdm(val_dataloader):\n",
        "    trained_perplexities.append(compute_perplexity(pretrained_lstm, batch))\n",
        "\n",
        "print(f\"Trained LSTM Perplexity: {torch.mean(torch.tensor(trained_perplexities))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4L3uZSz_dqSJ",
        "outputId": "78da6c5a-13d1-4ce2-f924-43852507f275"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'we propose a new recurrent architecture for semantically images can obtain the understanding of the k-means and a relations capable a neural network . multi-layer architectures make be highly subjective cannot size by up to the or underlying sequences . we demonstrate spectral scheme on the nonparametric sequence of previously data .'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sample(pretrained_lstm, \"We propose\", 50, sampling_strategy=sample_from_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Обучаемых параметров стало сильно меньше"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeUeobDny1lW"
      },
      "source": [
        "### [5 баллов] CharRNN Strikes Back\n",
        "\n",
        "Реализуйте модель, которая вместо токенов будет работать с символами. Как следует обучите её (не менее часа), посмотрите на перплексию и на предсказания, сравните с другими моделями. Сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9271064d0cd49239a3795ae197288d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "SOS_TOKEN = '[SOS]'\n",
        "EOS_TOKEN = '[EOS]'\n",
        "PAD_TOKEN = '[PAD]'\n",
        "\n",
        "char_vocabulary = defaultdict(lambda: len(char_vocabulary)) # Просто костыльное устранение проблемы недетерминизированности set\n",
        "_ = char_vocabulary[SOS_TOKEN]\n",
        "_ = char_vocabulary[EOS_TOKEN]\n",
        "_ = char_vocabulary[PAD_TOKEN]\n",
        "\n",
        "char_tokenized_texts = list()\n",
        "\n",
        "for text in tqdm(texts[:2000]):\n",
        "    # Токенизируем текст\n",
        "    char_tokenized_text = [token.lower() for token in text]\n",
        "    char_tokenized_text = [SOS_TOKEN] + char_tokenized_text + [EOS_TOKEN]\n",
        "\n",
        "    # Обновим словарь\n",
        "    for token in char_tokenized_text:\n",
        "         _ = char_vocabulary[token]\n",
        "\n",
        "    # Добавим токенизированный текст в датасет\n",
        "    char_tokenized_texts.append(char_tokenized_text)\n",
        "\n",
        "char_vocab_size = len(char_vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size is 72\n",
            "Tokenized example: ['[SOS]', 'w', 'e', ' ', 'p', 'r', 'o', 'p', 'o', 's', 'e', ' ', 'a', 'n', ' ', 'a', 'r', 'c', 'h', 'i', 't', 'e', 'c', 't', 'u', 'r', 'e', ' ', 'f', 'o', 'r', ' ', 'v', 'q', 'a', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'u', 't', 'i', 'l', 'i', 'z', 'e', 's', ' ', 'r', 'e', 'c', 'u', 'r', 'r', 'e', 'n', 't', ' ', 'l', 'a', 'y', 'e', 'r', 's', ' ', 't', 'o', '\\n', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e', ' ', 'v', 'i', 's', 'u', 'a', 'l', ' ', 'a', 'n', 'd', ' ', 't', 'e', 'x', 't', 'u', 'a', 'l', ' ', 'a', 't', 't', 'e', 'n', 't', 'i', 'o', 'n', '.', ' ', 't', 'h', 'e', ' ', 'm', 'e', 'm', 'o', 'r', 'y', ' ', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', 'i', 's', 't', 'i', 'c', ' ', 'o', 'f', ' ', 't', 'h', 'e', '\\n', 'p', 'r', 'o', 'p', 'o', 's', 'e', 'd', ' ', 'r', 'e', 'c', 'u', 'r', 'r', 'e', 'n', 't', ' ', 'a', 't', 't', 'e', 'n', 't', 'i', 'o', 'n', ' ', 'u', 'n', 'i', 't', 's', ' ', 'o', 'f', 'f', 'e', 'r', 's', ' ', 'a', ' ', 'r', 'i', 'c', 'h', ' ', 'j', 'o', 'i', 'n', 't', ' ', 'e', 'm', 'b', 'e', 'd', 'd', 'i', 'n', 'g', ' ', 'o', 'f', ' ', 'v', 'i', 's', 'u', 'a', 'l', ' ', 'a', 'n', 'd', '\\n', 't', 'e', 'x', 't', 'u', 'a', 'l', ' ', 'f', 'e', 'a', 't', 'u', 'r', 'e', 's', ' ', 'a', 'n', 'd', ' ', 'e', 'n', 'a', 'b', 'l', 'e', 's', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 't', 'o', ' ', 'r', 'e', 'a', 's', 'o', 'n', ' ', 'r', 'e', 'l', 'a', 't', 'i', 'o', 'n', 's', ' ', 'b', 'e', 't', 'w', 'e', 'e', 'n', ' ', 's', 'e', 'v', 'e', 'r', 'a', 'l', '\\n', 'p', 'a', 'r', 't', 's', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'i', 'm', 'a', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'q', 'u', 'e', 's', 't', 'i', 'o', 'n', '.', ' ', 'o', 'u', 'r', ' ', 's', 'i', 'n', 'g', 'l', 'e', ' ', 'm', 'o', 'd', 'e', 'l', ' ', 'o', 'u', 't', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 's', ' ', 't', 'h', 'e', ' ', 'f', 'i', 'r', 's', 't', ' ', 'p', 'l', 'a', 'c', 'e', '\\n', 'w', 'i', 'n', 'n', 'e', 'r', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'v', 'q', 'a', ' ', '1', '.', '0', ' ', 'd', 'a', 't', 'a', 's', 'e', 't', ',', ' ', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 's', ' ', 'w', 'i', 't', 'h', 'i', 'n', ' ', 'm', 'a', 'r', 'g', 'i', 'n', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'c', 'u', 'r', 'r', 'e', 'n', 't', '\\n', 's', 't', 'a', 't', 'e', '-', 'o', 'f', '-', 't', 'h', 'e', '-', 'a', 'r', 't', ' ', 'e', 'n', 's', 'e', 'm', 'b', 'l', 'e', ' ', 'm', 'o', 'd', 'e', 'l', '.', ' ', 'w', 'e', ' ', 'a', 'l', 's', 'o', ' ', 'e', 'x', 'p', 'e', 'r', 'i', 'm', 'e', 'n', 't', ' ', 'w', 'i', 't', 'h', ' ', 'r', 'e', 'p', 'l', 'a', 'c', 'i', 'n', 'g', ' ', 'a', 't', 't', 'e', 'n', 't', 'i', 'o', 'n', '\\n', 'm', 'e', 'c', 'h', 'a', 'n', 'i', 's', 'm', 's', ' ', 'i', 'n', ' ', 'o', 't', 'h', 'e', 'r', ' ', 's', 't', 'a', 't', 'e', '-', 'o', 'f', '-', 't', 'h', 'e', '-', 'a', 'r', 't', ' ', 'm', 'o', 'd', 'e', 'l', 's', ' ', 'w', 'i', 't', 'h', ' ', 'o', 'u', 'r', ' ', 'i', 'm', 'p', 'l', 'e', 'm', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', ' ', 'a', 'n', 'd', ' ', 's', 'h', 'o', 'w', '\\n', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'e', 'd', ' ', 'a', 'c', 'c', 'u', 'r', 'a', 'c', 'y', '.', ' ', 'i', 'n', ' ', 'b', 'o', 't', 'h', ' ', 'c', 'a', 's', 'e', 's', ',', ' ', 'o', 'u', 'r', ' ', 'r', 'e', 'c', 'u', 'r', 'r', 'e', 'n', 't', ' ', 'a', 't', 't', 'e', 'n', 't', 'i', 'o', 'n', ' ', 'm', 'e', 'c', 'h', 'a', 'n', 'i', 's', 'm', ' ', 'i', 'm', 'p', 'r', 'o', 'v', 'e', 's', '\\n', 'p', 'e', 'r', 'f', 'o', 'r', 'm', 'a', 'n', 'c', 'e', ' ', 'i', 'n', ' ', 't', 'a', 's', 'k', 's', ' ', 'r', 'e', 'q', 'u', 'i', 'r', 'i', 'n', 'g', ' ', 's', 'e', 'q', 'u', 'e', 'n', 't', 'i', 'a', 'l', ' ', 'o', 'r', ' ', 'r', 'e', 'l', 'a', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'r', 'e', 'a', 's', 'o', 'n', 'i', 'n', 'g', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'v', 'q', 'a', '\\n', 'd', 'a', 't', 'a', 's', 'e', 't', '.', '[EOS]']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Vocabulary size is {char_vocab_size}\")\n",
        "print(f\"Tokenized example: {char_tokenized_texts[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('[SOS]', 14)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_to_token = list(char_vocabulary) # id_to_token[i] -> token_i\n",
        "token_to_id = {token: id for id, token in enumerate(id_to_token)} # token_i -> i\n",
        "\n",
        "id_to_token[0], token_to_id['i']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d0304eb13f3493589c5b04e7910b3c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07f2b0913c5145849261df8493dcaa67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, val_texts = train_test_split(char_tokenized_texts, test_size=0.1,\n",
        "                                          random_state=42)\n",
        "\n",
        "train_dataset = TextsForLM(train_texts)\n",
        "val_dataset = TextsForLM(val_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_texts(batch):\n",
        "    max_length = 0\n",
        "    for text_ids in batch:\n",
        "        max_length = max(max_length, len(text_ids))\n",
        "\n",
        "    for i in range(len(batch)):\n",
        "        batch[i] += [token_to_id[PAD_TOKEN]] * (max_length - len(batch[i]))\n",
        "\n",
        "    return torch.LongTensor(batch)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2,\n",
        "                              collate_fn=collate_texts)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2,\n",
        "                            collate_fn=collate_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "from typing import Union\n",
        "\n",
        "class LMModel(LightningModule):\n",
        "    def __init__(self, vocab_size, emb_dim=128, rnn_hidden_dim=128,\n",
        "                 rnn_num_layers=2, RNN: Union[nn.RNN, nn.LSTM, nn.GRU] = nn.LSTM):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = RNN(input_size=emb_dim, hidden_size=rnn_hidden_dim,\n",
        "                       batch_first=True, num_layers=rnn_num_layers)\n",
        "        self.output_layer = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.embedding_layer(input_ids)\n",
        "\n",
        "        output, state = self.rnn(embeddings)\n",
        "\n",
        "        logits = self.output_layer(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        logits, state = self.forward(batch)\n",
        "\n",
        "        batch_size, seq_len, vocab_size = logits.shape\n",
        "\n",
        "        pred = logits[:, :-1, :].reshape(batch_size * (seq_len - 1), vocab_size)\n",
        "        target = batch[:, 1:].reshape(batch_size * (seq_len - 1))\n",
        "\n",
        "        loss_fn = nn.CrossEntropyLoss(ignore_index=token_to_id[PAD_TOKEN])\n",
        "\n",
        "        loss = loss_fn(pred, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=3e-3)\n",
        "\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "char_lstm = LMModel(char_vocab_size, emb_dim=256, rnn_hidden_dim=256, RNN=nn.LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "max_epochs = 50\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name            | Type      | Params\n",
            "----------------------------------------------\n",
            "0 | embedding_layer | Embedding | 18.4 K\n",
            "1 | rnn             | LSTM      | 1.1 M \n",
            "2 | output_layer    | Linear    | 18.5 K\n",
            "----------------------------------------------\n",
            "1.1 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.1 M     Total params\n",
            "4.358     Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b26d189ee5c74f2fb97b70efabc05aac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "    Exception ignored in: if w.is_alive():<function _MultiProcessingDataLoaderIter.__del__ at 0x79f8ff50c4c0>\n",
            "\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "Traceback (most recent call last):\n",
            "      File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
            "self._shutdown_workers()AssertionError: can only test a child process  File \"/home/semyon/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
            "\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(devices=1, accelerator=\"gpu\", max_epochs=max_epochs, log_every_n_steps=1)\n",
        "trainer.fit(char_lstm, train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.save(char_lstm.state_dict(), 'model/char_lstm.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# char_lstm.load_state_dict(torch.load('model/char_lstm.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c201bef248642c3ae2e62150e355d4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained char LSTM Perplexity: 2.006401538848877\n"
          ]
        }
      ],
      "source": [
        "trained_perplexities = []\n",
        "\n",
        "for batch in tqdm(val_dataloader):\n",
        "    trained_perplexities.append(compute_perplexity(char_lstm, batch))\n",
        "\n",
        "print(f\"Trained char LSTM Perplexity: {torch.mean(torch.tensor(trained_perplexities))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sample(model, beginning, max_length,\n",
        "                    sampling_strategy=sample_from_distribution,\n",
        "                    temperature=1.0):\n",
        "    if beginning is None:\n",
        "        tokens = [token_to_id[SOS_TOKEN]]\n",
        "    else:\n",
        "        tokens = [token_to_id[token] for token in beginning.lower()]\n",
        "        tokens = [token_to_id[SOS_TOKEN]] + tokens\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        tokens_tensor = torch.LongTensor(tokens).unsqueeze(0)\n",
        "\n",
        "        logits, _ = model(tokens_tensor)\n",
        "\n",
        "        logits = logits / temperature\n",
        "\n",
        "        next_token_id = sampling_strategy(logits)\n",
        "\n",
        "        if next_token_id == token_to_id[EOS_TOKEN]:\n",
        "            break\n",
        "\n",
        "        tokens.append(next_token_id)\n",
        "\n",
        "    generated_sample = ' '.join([id_to_token[id] for id in tokens[1:]])\n",
        "\n",
        "    return generated_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'w e   p r o p o s e   a n   e x t e n s i o n   o f   u s e r s \\n w i t h   t h e   g o a l   i n   c o m p u t a t i o n a l   c o m p u t a t i o n ,   i m a g e s   i n   d e e p   l e a r n i n g .   i t   i s   m o'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sample(char_lstm, beginning='We propose', max_length=100, sampling_strategy=sample_from_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "В целом слова он генерирует более менее связно, но текст получается бредовый."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnPH1c8BZ71h"
      },
      "source": [
        "### [6 баллов] Стратегии сэмплирования\n",
        "https://huggingface.co/blog/how-to-generate\n",
        "\n",
        "Реализуйте температурный софтмакс (1 балл).\n",
        "\n",
        "Реализуйте sample_top_k_from_distribution, nucleus_sampling (по 1 баллу).\n",
        "\n",
        "Реализуйте beam search (2 балла) и придумайте, как его можно совместить с сэмплированием (Beam-search multinomial sampling). (+1 балл)\n",
        "\n",
        "Сравните различные подходы и комбинации подходов (сделайте таблицу результатов), сделайте выводы (-2 балла, если отсутствует)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "def beam_search(model, tokens, beam_size=10, max_length=3, temperature=1.0):\n",
        "    suffixes = [[]]\n",
        "    probs = [1]\n",
        "\n",
        "    tokens_tensor = torch.LongTensor(tokens).unsqueeze(0)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        next_suffixes = []\n",
        "        next_probs = []\n",
        "        for i in range(len(suffixes)):\n",
        "            if len(suffixes[i]) > 0 and suffixes[i][-1] == token_to_id[EOS_TOKEN]:\n",
        "                continue\n",
        "\n",
        "            logits, _ = model(torch.cat((tokens_tensor, torch.LongTensor(suffixes[i]).unsqueeze(0)), dim=-1))\n",
        "\n",
        "            logits = logits / temperature\n",
        "\n",
        "            top_k_probs, idx = torch.nn.functional.softmax(logits[0, -1], dim=-1).topk(beam_size, dim=-1)\n",
        "\n",
        "            for j in range(beam_size):\n",
        "                next_suffixes.append(torch.cat((torch.LongTensor(suffixes[i]), torch.LongTensor(idx[j]).reshape(-1,)), dim=0).tolist())\n",
        "                next_probs.append((top_k_probs[j] * probs[i]).tolist())\n",
        "        \n",
        "        suffixes = next_suffixes\n",
        "        probs = next_probs\n",
        "    \n",
        "    return suffixes[torch.Tensor(probs).argmax()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_sample(model, beginning, max_length,\n",
        "                    sampling_strategy=sample_from_distribution,\n",
        "                    temperature=1.0):\n",
        "    if beginning is None:\n",
        "        tokens = [token_to_id[SOS_TOKEN]]\n",
        "    else:\n",
        "        tokens = [token_to_id[token.text] for token in tokenize(beginning.lower())]\n",
        "        tokens = [token_to_id[SOS_TOKEN]] + tokens\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        if sampling_strategy == beam_search:\n",
        "            tokens += beam_search(model, tokens, temperature=temperature)\n",
        "\n",
        "            if tokens[-1] == token_to_id[EOS_TOKEN]:\n",
        "                tokens.pop()\n",
        "                break\n",
        "        else:\n",
        "            tokens_tensor = torch.LongTensor(tokens).unsqueeze(0)\n",
        "\n",
        "            logits, _ = model(tokens_tensor)\n",
        "\n",
        "            logits = logits / temperature\n",
        "\n",
        "            next_token_id = sampling_strategy(logits)\n",
        "\n",
        "            if next_token_id == token_to_id[EOS_TOKEN]:\n",
        "                break\n",
        "\n",
        "            tokens.append(next_token_id)\n",
        "\n",
        "    generated_sample = ' '.join([id_to_token[id] for id in tokens[1:]])\n",
        "\n",
        "    return generated_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_model = LMModel(vocab_size, RNN=nn.LSTM)\n",
        "lstm_model.load_state_dict(torch.load('model/longer_lstm.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'we propose a novel method for deep learning , where the representations are selected compared to the recently proposed framework to computer vision . in this paper , we propose a novel factorization-based model of rpca , which we derive a large number of time-frames .'"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sample(lstm_model, \"We propose\", 50, sampling_strategy=beam_search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueXb5F4NHnbv"
      },
      "source": [
        "# Библиография\n",
        "\n",
        "Если есть желание ещё лучше разобраться в теме, можно изучить следующие материалы:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apLia0ACblhS"
      },
      "source": [
        "1. Блогпост Андрея Карпати про CharRNN: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "2. Глава в учебнике Лены Войты: https://lena-voita.github.io/nlp_course/language_modeling.html\n",
        "\n",
        "3. Статья про генерацию текста в произвольной последовательности: https://arxiv.org/pdf/2102.11008.pdf\n",
        "\n",
        "4. Nucleus Sampling: https://openreview.net/pdf?id=rygGQyrFvH"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "co-TRbmQFuHm",
        "2eK1sdmfGz7v",
        "O8JjPd88GxJ1",
        "xxB7ucBaTQX3",
        "lEEIgJ2CTTl9",
        "2MZbJZsFZ0sh",
        "0CYlhS7iw-5D",
        "ueXb5F4NHnbv"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "006e44e70c5a47c4a0fc0c78ee71565c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d51c534e50e54fc0999af992af3bbd9a",
              "IPY_MODEL_bab78181cd9149c3a0de6aebf868cc0e",
              "IPY_MODEL_992eabdf589e43d99137f5a78d02c3ae"
            ],
            "layout": "IPY_MODEL_13aaf60a13ed4f1d970d264ed1b1522e"
          }
        },
        "0490df77771b40e59e276420bb00e277": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "094db87adee04b5986a96e8562cee86d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e20ae2bd7694b2c84d4e581d5fdf167": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f15f5a90670454581235e168f4bcef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10babac1cdf14c4bb7baefabd85d2bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13aaf60a13ed4f1d970d264ed1b1522e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19f5e3f251f34fa9a2ea25d355ecde37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2097ebffe50a4cd1981f4472bdeee03c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26df445ae4d84282ad44693c66670c06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b291a4984494a7e84d5dc63fe4d6550": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48341429b47a45dca6648305edcdba18",
            "placeholder": "​",
            "style": "IPY_MODEL_f9652212bf4247beb4868fd5361b1677",
            "value": " 1800/1800 [00:00&lt;00:00, 22145.54it/s]"
          }
        },
        "2ddae9e843434bb6949db73f423b2b2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2097ebffe50a4cd1981f4472bdeee03c",
            "placeholder": "​",
            "style": "IPY_MODEL_663df831b8234b4086156a2d9c24ce08",
            "value": "100%"
          }
        },
        "3bd9b28c7bf843e5a1316ee1f0a43c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d7e272d08354db3a8430eb8f2d541df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e20ae2bd7694b2c84d4e581d5fdf167",
            "placeholder": "​",
            "style": "IPY_MODEL_0f15f5a90670454581235e168f4bcef3",
            "value": " 50/50 [00:07&lt;00:00,  6.72it/s]"
          }
        },
        "48341429b47a45dca6648305edcdba18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b42441d6f714ca2859526a5f520c997": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a16b50c7b966445aa36036a479c857e1",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81c9a8fd01864e18923fce605d65c00d",
            "value": 50
          }
        },
        "4ed06b6082a74e0fae7b4b1465962fea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c8593cda204ff08bc328d004d78f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ddae9e843434bb6949db73f423b2b2a",
              "IPY_MODEL_4b42441d6f714ca2859526a5f520c997",
              "IPY_MODEL_3d7e272d08354db3a8430eb8f2d541df"
            ],
            "layout": "IPY_MODEL_b8548d63d0d94f8f949cebb76c47cd97"
          }
        },
        "57ef160499b3489a806512ee5ec0f627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58cb5be0dce34ad3afb27ea937c7e367": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be4aa46e67f24f12af4015350a0f05f6",
              "IPY_MODEL_dac4dc26dba6458abc1d83a8a60b0510",
              "IPY_MODEL_2b291a4984494a7e84d5dc63fe4d6550"
            ],
            "layout": "IPY_MODEL_7b4b7ffdd0444fd9bbf9ebc5bbaee1be"
          }
        },
        "58fae58d570146b4939057326245056b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_679226d3274642a3a4ca9902c2b53409",
            "placeholder": "​",
            "style": "IPY_MODEL_10babac1cdf14c4bb7baefabd85d2bac",
            "value": "Epoch 19: 100%"
          }
        },
        "650b89acbe214799a467de340af4cd33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "663df831b8234b4086156a2d9c24ce08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "679226d3274642a3a4ca9902c2b53409": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b4b7ffdd0444fd9bbf9ebc5bbaee1be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d806c10dcb84167998d2891eefd7d30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81c9a8fd01864e18923fce605d65c00d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85c4528329cd46a48c8c245249ac14fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cdb6b27f432413d872cf1788a1e6ea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d36edbc3caa4167bc6ab2556d8ad343": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93c87e247e4d4f259b8b9a829b4d2a71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95e8554bc7cf46e7b7ce415eea6093ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d806c10dcb84167998d2891eefd7d30",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f464fdaaf0f94587a7d3f8720e8ea30e",
            "value": 200
          }
        },
        "992eabdf589e43d99137f5a78d02c3ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0490df77771b40e59e276420bb00e277",
            "placeholder": "​",
            "style": "IPY_MODEL_26df445ae4d84282ad44693c66670c06",
            "value": " 2000/2000 [00:03&lt;00:00, 936.13it/s]"
          }
        },
        "a14eab98f8b94c31b7e46cfd6f826094": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a16b50c7b966445aa36036a479c857e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a35a5b65674a45fea95d606810c408ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8e20595d05c4955b981ac108e16804b",
            "placeholder": "​",
            "style": "IPY_MODEL_b477235ea336406e88ef141e0e5c7962",
            "value": "100%"
          }
        },
        "b09efefa759b40dc9ecc9d205c5c92b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a35a5b65674a45fea95d606810c408ad",
              "IPY_MODEL_95e8554bc7cf46e7b7ce415eea6093ef",
              "IPY_MODEL_ec439af9d33249ba933b820cd1d69e1e"
            ],
            "layout": "IPY_MODEL_a14eab98f8b94c31b7e46cfd6f826094"
          }
        },
        "b477235ea336406e88ef141e0e5c7962": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b568a0a78ba745199c242119377dd708": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "b8548d63d0d94f8f949cebb76c47cd97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bab78181cd9149c3a0de6aebf868cc0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1264477eec34056af39bd97ad70796b",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19f5e3f251f34fa9a2ea25d355ecde37",
            "value": 2000
          }
        },
        "bc77ea3b44ae421cae5355069178ec69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce16e7ff23b54cf3a1f70a81dbf778f2",
            "max": 450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bd9b28c7bf843e5a1316ee1f0a43c8f",
            "value": 450
          }
        },
        "be4aa46e67f24f12af4015350a0f05f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e258c1834c154ef0afad4343b80d4121",
            "placeholder": "​",
            "style": "IPY_MODEL_650b89acbe214799a467de340af4cd33",
            "value": "100%"
          }
        },
        "c4f2674717954f1e9a05a1087f73cc7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58fae58d570146b4939057326245056b",
              "IPY_MODEL_bc77ea3b44ae421cae5355069178ec69",
              "IPY_MODEL_ccf9275a12504679baf786a9867a20ba"
            ],
            "layout": "IPY_MODEL_b568a0a78ba745199c242119377dd708"
          }
        },
        "ccf9275a12504679baf786a9867a20ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ed06b6082a74e0fae7b4b1465962fea",
            "placeholder": "​",
            "style": "IPY_MODEL_094db87adee04b5986a96e8562cee86d",
            "value": " 450/450 [00:05&lt;00:00, 76.13it/s, v_num=1]"
          }
        },
        "ce16e7ff23b54cf3a1f70a81dbf778f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d51c534e50e54fc0999af992af3bbd9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85c4528329cd46a48c8c245249ac14fc",
            "placeholder": "​",
            "style": "IPY_MODEL_57ef160499b3489a806512ee5ec0f627",
            "value": "100%"
          }
        },
        "d66a86b0b8494156b381247edd850028": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dac4dc26dba6458abc1d83a8a60b0510": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d36edbc3caa4167bc6ab2556d8ad343",
            "max": 1800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cdb6b27f432413d872cf1788a1e6ea1",
            "value": 1800
          }
        },
        "e1264477eec34056af39bd97ad70796b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e258c1834c154ef0afad4343b80d4121": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec439af9d33249ba933b820cd1d69e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93c87e247e4d4f259b8b9a829b4d2a71",
            "placeholder": "​",
            "style": "IPY_MODEL_d66a86b0b8494156b381247edd850028",
            "value": " 200/200 [00:00&lt;00:00, 8113.95it/s]"
          }
        },
        "f464fdaaf0f94587a7d3f8720e8ea30e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8e20595d05c4955b981ac108e16804b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9652212bf4247beb4868fd5361b1677": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
